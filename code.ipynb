{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords as StopwordsLoader\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To evaluate my code on additional data, the file locations should be entered into \n",
    "#the list “filenames”. \n",
    "#Then, the whole notebook should be run, with the results being output in the final two cells. \n",
    "\n",
    "filenames = [\"semeval-tweets/twitter-test1.txt\", \"semeval-tweets/twitter-test2.txt\", \"semeval-tweets/twitter-test3.txt\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/a226/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('semeval-tweets/twitter-training-data.txt', sep=\"\\t\", names=[\"id\", \"sentiment\", \"tweet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_set(filename):\n",
    "    list_of_words = []\n",
    "    with open(filename,\"r\", encoding='latin-1') as f:\n",
    "        for line in f.readlines():\n",
    "            tmp = line.rstrip(\"\\n\")\n",
    "            list_of_words.append(tmp)\n",
    "    return set(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positivewords = load_set(\"positive-words.txt\")\n",
    "negativewords = load_set(\"negative-words.txt\")\n",
    "badwords = load_set(\"bad-words.txt\")\n",
    "negationwords = {\"no\", \"not\",\"none\",\"nobody\",\"nothing\",\"neither\",\"nowhere\",\n",
    "                    \"never\",\"hardly\",\"scarcely\",\"barely\",\"doesnt\",\"isnt\",\"wasnt\",\n",
    "                    \"shouldnt\",\"wouldnt\",\"couldnt\",\"wont\",\"cant\",\"dont\",\"arent\",\"amnt\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regexp(data):    \n",
    "    url_pattern = re.compile(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")\n",
    "    an_pattern = re.compile(r\"[.,\\!$%\\^&\\*\\(\\)=\\-_`~\\+\\{\\}<>\\?:;\\'\\\"\\[\\]\\|]\")\n",
    "    sl_pattern = re.compile(r\"(\\b\\w\\b)\")\n",
    "    pos_em_pattern = re.compile(r':\\)|:]|:3|:>|8\\)|\\(:|=\\)|=]|:\\'\\)|:-\\)')\n",
    "    neg_em_pattern = re.compile(r':\\(|:\\[|:<|8\\(|\\):|=\\(|=\\[|:\\'\\(|:-\\(')\n",
    "    usermention_pattern = re.compile(r'^(?!.*\\bRT\\b)(?:.+\\s)?@\\w+')\n",
    "    laugh_pattern = re.compile(r\"\\b(?:a*(?:ha)+h?|h*ha+h[ha]*|(?:l+o+)+l+|o?l+o+l+[ol]*)\\b\")\n",
    "    pattern_elonganted = re.compile(r'(.)\\1+')\n",
    "    number_pattern = re.compile(r\"\\w*\\d\\w*\")\n",
    "    for i in range(len(data)):\n",
    "        st = pos_em_pattern.sub('POSEMOJY', data['tweet'][i])\n",
    "        st = neg_em_pattern.sub('NEGEMOJY', st)\n",
    "        st = usermention_pattern.sub('USERMENTION', st)\n",
    "        st = laugh_pattern.sub('LAUGHINTWEET', st)\n",
    "        st = url_pattern.sub(\"URLINTWEET\", st)\n",
    "        st = number_pattern.sub(\"\", st )\n",
    "        st = pattern_elonganted.sub(r'\\1\\1',st)\n",
    "        st = an_pattern.sub(\" \", st)\n",
    "        st = sl_pattern.sub(\"\", st)\n",
    "        st = st.replace(\"/\", \" \")\n",
    "        st = st.replace(\"\\\\\", \" \")\n",
    "        data['tweet'] = data['tweet'].replace(data['tweet'][i], st)       \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(data):\n",
    "    text_tokens = []\n",
    "    tt = TweetTokenizer()\n",
    "    for text in data['tweet']:\n",
    "        text = text.lower()\n",
    "        text = text.encode(\"ascii\", \"ignore\")\n",
    "        text_tokens.append(tt.tokenize(text))   \n",
    "    return text_tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    sentences = []\n",
    "    for tokens in data:\n",
    "        sentences.append(lemsentence(tokens))   \n",
    "    return sentences\n",
    "\n",
    "def lemsentence(tokens):\n",
    "    lemm = nltk.stem.WordNetLemmatizer()\n",
    "    stop_words = StopwordsLoader.words(\"English\")\n",
    "    sentence = []\n",
    "    for word, tag in nltk.tag.pos_tag(tokens):\n",
    "        if word.lower() in stop_words:\n",
    "            continue\n",
    "        if word.lower() in negationwords:\n",
    "            word = \"negationintweet\"\n",
    "        if tag.startswith('N'):\n",
    "            pos = nltk.corpus.wordnet.NOUN\n",
    "        elif tag.startswith('V'):\n",
    "            pos = nltk.corpus.wordnet.VERB\n",
    "        else:\n",
    "            pos = nltk.corpus.wordnet.ADJ\n",
    "        \n",
    "        st = lemm.lemmatize(word, pos).lower()\n",
    "        sentence.append(st)\n",
    "    return sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(tweet, words_set):\n",
    "    count = 0\n",
    "    for word in tweet:\n",
    "        if word in words_set:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_vector(tweet):\n",
    "    res = []\n",
    "    res.append(counter(tweet,positivewords))\n",
    "    res.append(counter(tweet,negativewords))\n",
    "    res.append(counter(tweet,badwords))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_dataset(dataset):\n",
    "    dataset = regexp(dataset)\n",
    "    dataset[\"tokens\"] = tokenization(dataset)\n",
    "    dataset[\"tokens\"] = lemmatization(dataset[\"tokens\"])\n",
    "    corpus = []\n",
    "    counts = []\n",
    "    for tweet in dataset[\"tokens\"]:\n",
    "        counts.append(create_feature_vector(tweet))\n",
    "        corpus.append(' '.join(tweet))\n",
    "    df = pd.DataFrame(counts, columns = ['PositiveWords', 'NegativeWords', 'BadWords']) \n",
    "    return corpus, df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45026\n"
     ]
    }
   ],
   "source": [
    "print(len(train['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata, traincount_df = handle_dataset(train)\n",
    "train_y = train['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['abortion', 'abuse', 'ac', 'aleppo', 'allow', 'alt', 'amaze', 'amazing',\n",
      "       'amazon', 'amendment',\n",
      "       ...\n",
      "       'xx', 'xxl', 'yakub', 'yay', 'yoga', 'york', 'zac', 'PositiveWords',\n",
      "       'NegativeWords', 'BadWords'],\n",
      "      dtype='object', length=503)\n",
      "Index(['abortion', 'ac', 'aleppo', 'allow', 'alt', 'amaze', 'amazing',\n",
      "       'amazon', 'amendment', 'america',\n",
      "       ...\n",
      "       'wtf', 'xx', 'xxl', 'yakub', 'yay', 'yoga', 'zac', 'PositiveWords',\n",
      "       'NegativeWords', 'BadWords'],\n",
      "      dtype='object', length=503)\n"
     ]
    }
   ],
   "source": [
    "# build CountVectorizer on train \n",
    "cv = CountVectorizer()\n",
    "cv_train_features = cv.fit_transform(traindata)\n",
    "traincv_df = pd.DataFrame(cv_train_features.toarray(),columns=list(cv.get_feature_names()))\n",
    "ch2cv = SelectKBest(chi2, k=500)\n",
    "X_traincv = ch2cv.fit_transform(traincv_df, train_y)\n",
    "cols = ch2cv.get_support(indices=True)\n",
    "featurescv = traincv_df.columns\n",
    "featurescv_k = []\n",
    "for i in cols:\n",
    "    featurescv_k.append(featurescv[i])\n",
    "X_traincv = pd.DataFrame(X_traincv,columns = featurescv_k)\n",
    "X_traincv = pd.concat([X_traincv, traincount_df],  axis=1)\n",
    "print(X_traincv.columns)\n",
    "# build TFIDF features on train \n",
    "tv = TfidfVectorizer()\n",
    "tv_train_features = tv.fit_transform(traindata)\n",
    "traintv_df = pd.DataFrame(tv_train_features.toarray(),columns=list(tv.get_feature_names()))\n",
    "ch2tv = SelectKBest(chi2, k=500)\n",
    "X_traintv = ch2tv.fit_transform(traintv_df, train_y)\n",
    "cols = ch2tv.get_support(indices=True)\n",
    "featurestv = traintv_df.columns\n",
    "featurestv_k = []\n",
    "for i in cols:\n",
    "    featurestv_k.append(featurestv[i])\n",
    "X_traintv = pd.DataFrame(X_traintv,columns = featurestv_k)\n",
    "X_traintv = pd.concat([X_traintv, traincount_df],  axis=1)\n",
    "print(X_traintv.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV model features shape: (45026, 503)\n",
      "TFIDF model features shape: (45026, 503)\n"
     ]
    }
   ],
   "source": [
    "print('CV model features shape:', X_traincv.shape)\n",
    "print('TFIDF model features shape:', X_traintv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_tv = X_traintv\n",
    "traindata_cv = X_traincv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "gnb = GaussianNB()\n",
    "pac = PassiveAggressiveClassifier(C=0.0001, loss='squared_hinge')\n",
    "classifiers = [mnb, gnb, pac]\n",
    "classifiers_names = [\"MultinomialNB\",\"GaussianNB\",\"PassiveAggressiveClassifier\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPpeparation(FILENAME, flag = 1):\n",
    "    test = pd.read_csv(FILENAME, sep=\"\\t\", names=[\"id\", \"sentiment\", \"tweet\"]\n",
    "           )\n",
    "    test_id = test['id']\n",
    "    test1_y = test['sentiment']\n",
    "    test, testcount_df  = handle_dataset(test)   \n",
    "    if flag == 2:\n",
    "        return test, test1_y, test_id\n",
    "    cv = CountVectorizer(vocabulary = featurescv_k)\n",
    "    tv = TfidfVectorizer(vocabulary = featurestv_k)\n",
    "    cvtest_features = (cv.fit_transform(test)).toarray()\n",
    "    tvtest_features = (tv.fit_transform(test)).toarray()\n",
    "    test_tv = pd.DataFrame(tvtest_features, columns=list(tv.get_feature_names()))\n",
    "    test_tv = pd.concat([test_tv, testcount_df], axis = 1)\n",
    "    test_cv = pd.DataFrame(cvtest_features, columns=list(cv.get_feature_names()))\n",
    "    test_cv = pd.concat([test_cv, testcount_df], axis = 1)\n",
    "    if flag == 1:\n",
    "        return test_tv, test_cv, test1_y\n",
    "    else: \n",
    "        return test_tv, test_cv, test1_y, test_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_metrics(true_labels, predicted_labels):\n",
    "    print('Accuracy:', np.round(\n",
    "                        metrics.accuracy_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        4))\n",
    "    print('F1 Score:', np.round(\n",
    "                        metrics.f1_score(true_labels, \n",
    "                                               predicted_labels,\n",
    "                                               average='weighted'),\n",
    "                        4))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_model(classifier, \n",
    "                        train_features, train_labels, \n",
    "                        test_features, test_labels):\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    predictions = classifier.predict(test_features) \n",
    "    return predictions    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semeval-tweets/twitter-test1.txt\n",
      "CVMultinomialNB\n",
      "Accuracy: 0.629\n",
      "F1 Score: 0.6194\n",
      "TVMultinomialNB\n",
      "Accuracy: 0.6182\n",
      "F1 Score: 0.5995\n",
      "CVGaussianNB\n",
      "Accuracy: 0.4594\n",
      "F1 Score: 0.3519\n",
      "TVGaussianNB\n",
      "Accuracy: 0.4721\n",
      "F1 Score: 0.3817\n",
      "CVPassiveAggressiveClassifier\n",
      "Accuracy: 0.6497\n",
      "F1 Score: 0.6298\n",
      "TVPassiveAggressiveClassifier\n",
      "Accuracy: 0.5811\n",
      "F1 Score: 0.5602\n",
      "semeval-tweets/twitter-test2.txt\n",
      "CVMultinomialNB\n",
      "Accuracy: 0.6357\n",
      "F1 Score: 0.6355\n",
      "TVMultinomialNB\n",
      "Accuracy: 0.6141\n",
      "F1 Score: 0.6077\n",
      "CVGaussianNB\n",
      "Accuracy: 0.5364\n",
      "F1 Score: 0.4183\n",
      "TVGaussianNB\n",
      "Accuracy: 0.5532\n",
      "F1 Score: 0.4652\n",
      "CVPassiveAggressiveClassifier\n",
      "Accuracy: 0.6336\n",
      "F1 Score: 0.6296\n",
      "TVPassiveAggressiveClassifier\n",
      "Accuracy: 0.6125\n",
      "F1 Score: 0.6026\n",
      "semeval-tweets/twitter-test3.txt\n",
      "CVMultinomialNB\n",
      "Accuracy: 0.5956\n",
      "F1 Score: 0.5914\n",
      "TVMultinomialNB\n",
      "Accuracy: 0.5851\n",
      "F1 Score: 0.5708\n",
      "CVGaussianNB\n",
      "Accuracy: 0.4704\n",
      "F1 Score: 0.3742\n",
      "TVGaussianNB\n",
      "Accuracy: 0.4834\n",
      "F1 Score: 0.4071\n",
      "CVPassiveAggressiveClassifier\n"
     ]
    }
   ],
   "source": [
    "for filee in filenames:\n",
    "    print(filee)\n",
    "    test_tv, test_cv, test_y = testPpeparation(filee)\n",
    "    for i in range(len(classifiers)):\n",
    "        print(\"CV\" + classifiers_names[i])\n",
    "        get_metrics(test_y, train_predict_model(classifiers[i], traindata_cv, train_y, test_cv, test_y ) )\n",
    "        print(\"TV\" + classifiers_names[i])\n",
    "        get_metrics(test_y, train_predict_model(classifiers[i], traindata_tv, train_y, test_tv, test_y ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 101 \n",
    "np.random.seed(seed)\n",
    "\n",
    "def change(x):\n",
    "    arr = []\n",
    "    for elem in x:\n",
    "        if elem == \"positive\":\n",
    "            arr.append(2)\n",
    "        if elem == \"negative\":\n",
    "            arr.append(1)\n",
    "        if elem == \"neutral\":\n",
    "            arr.append(0)\n",
    "    return np.asarray(arr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "X_train = traindata\n",
    "max_features = 5000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "Y_train = change(train_y)\n",
    "max_words = 50 \n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "Y_train = to_categorical(Y_train, 3)\n",
    "batch_size = 128\n",
    "epochs = 16\n",
    "def get_model(max_features, embed_dim, embedding_matrix):\n",
    "    np.random.seed(seed)\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1],\n",
    "                       weights=[embedding_matrix],trainable=False))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "def get_embed_mat(EMBEDDING_FILE, max_features=5000):\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf8'))\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_features, len(word_index) + 1)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n",
    "                                        (num_words, embed_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    max_features = embedding_matrix.shape[0]\n",
    "    \n",
    "    return max_features, embedding_matrix\n",
    "    \n",
    "EMBEDDING_FILE = 'glove.6b/glove.6B.100d.txt'\n",
    "embed_dim = 100 \n",
    "max_features, embedding_matrix = get_embed_mat(EMBEDDING_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 100)           500000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 580,703\n",
      "Trainable params: 80,703\n",
      "Non-trainable params: 500,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/16\n",
      "352/352 [==============================] - 75s 214ms/step - loss: 0.9016 - accuracy: 0.5561\n",
      "Epoch 2/16\n",
      "352/352 [==============================] - 69s 196ms/step - loss: 0.8550 - accuracy: 0.5947\n",
      "Epoch 3/16\n",
      "352/352 [==============================] - 64s 182ms/step - loss: 0.8317 - accuracy: 0.6064\n",
      "Epoch 4/16\n",
      "352/352 [==============================] - 74s 209ms/step - loss: 0.8125 - accuracy: 0.6198\n",
      "Epoch 5/16\n",
      "352/352 [==============================] - 59s 167ms/step - loss: 0.7987 - accuracy: 0.6282\n",
      "Epoch 6/16\n",
      "352/352 [==============================] - 59s 167ms/step - loss: 0.7855 - accuracy: 0.6350\n",
      "Epoch 7/16\n",
      "352/352 [==============================] - 59s 167ms/step - loss: 0.7745 - accuracy: 0.6421\n",
      "Epoch 8/16\n",
      "352/352 [==============================] - 61s 172ms/step - loss: 0.7636 - accuracy: 0.6458\n",
      "Epoch 9/16\n",
      "352/352 [==============================] - 60s 170ms/step - loss: 0.7543 - accuracy: 0.6535\n",
      "Epoch 10/16\n",
      "352/352 [==============================] - 59s 168ms/step - loss: 0.7449 - accuracy: 0.6572\n",
      "Epoch 11/16\n",
      "352/352 [==============================] - 60s 169ms/step - loss: 0.7361 - accuracy: 0.6624\n",
      "Epoch 12/16\n",
      "352/352 [==============================] - 59s 169ms/step - loss: 0.7274 - accuracy: 0.6690\n",
      "Epoch 13/16\n",
      "352/352 [==============================] - 59s 167ms/step - loss: 0.7188 - accuracy: 0.6742\n",
      "Epoch 14/16\n",
      "352/352 [==============================] - 58s 165ms/step - loss: 0.7110 - accuracy: 0.6759\n",
      "Epoch 15/16\n",
      "352/352 [==============================] - 59s 168ms/step - loss: 0.7033 - accuracy: 0.6805\n",
      "Epoch 16/16\n",
      "352/352 [==============================] - 59s 168ms/step - loss: 0.6963 - accuracy: 0.6858\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_model(max_features, embed_dim, embedding_matrix)\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 16ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.79      0.68      1504\n",
      "           1       0.70      0.32      0.44       557\n",
      "           2       0.72      0.64      0.68      1470\n",
      "\n",
      "    accuracy                           0.65      3531\n",
      "   macro avg       0.67      0.58      0.60      3531\n",
      "weighted avg       0.67      0.65      0.64      3531\n",
      "\n",
      "58/58 [==============================] - 1s 16ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.75      0.62       669\n",
      "           1       0.71      0.30      0.42       202\n",
      "           2       0.76      0.65      0.70       982\n",
      "\n",
      "    accuracy                           0.65      1853\n",
      "   macro avg       0.67      0.57      0.58      1853\n",
      "weighted avg       0.68      0.65      0.64      1853\n",
      "\n",
      "75/75 [==============================] - 1s 18ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.80      0.66       983\n",
      "           1       0.64      0.34      0.44       363\n",
      "           2       0.71      0.55      0.62      1033\n",
      "\n",
      "    accuracy                           0.62      2379\n",
      "   macro avg       0.64      0.56      0.58      2379\n",
      "weighted avg       0.64      0.62      0.61      2379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "    X_test, y_test, y_id = testPpeparation(filename, 2)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "    y_test = change(y_test)\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TV\n",
      "semeval-tweets/twitter-test1.txt (MultinomialNB): 0.457\n",
      "            positive  negative  neutral\n",
      "positive    0.694     0.058     0.248     \n",
      "negative    0.121     0.686     0.193     \n",
      "neutral     0.273     0.182     0.546     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test1.txt (MultinomialNB): 0.513\n",
      "            positive  negative  neutral\n",
      "positive    0.682     0.060     0.258     \n",
      "negative    0.137     0.641     0.222     \n",
      "neutral     0.272     0.166     0.563     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test1.txt (GaussianNB): 0.507\n",
      "            positive  negative  neutral\n",
      "positive    0.477     0.100     0.423     \n",
      "negative    0.175     0.450     0.375     \n",
      "neutral     0.336     0.178     0.486     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test1.txt (GaussianNB): 0.433\n",
      "            positive  negative  neutral\n",
      "positive    0.451     0.125     0.423     \n",
      "negative    0.176     0.562     0.261     \n",
      "neutral     0.304     0.199     0.497     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test1.txt (PassiveAggressiveClassifier): 0.451\n",
      "            positive  negative  neutral\n",
      "positive    0.588     0.089     0.323     \n",
      "negative    0.143     0.636     0.221     \n",
      "neutral     0.271     0.182     0.547     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test1.txt (PassiveAggressiveClassifier): 0.475\n",
      "            positive  negative  neutral\n",
      "positive    0.778     0.065     0.157     \n",
      "negative    0.160     0.704     0.136     \n",
      "neutral     0.279     0.160     0.561     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test2.txt (MultinomialNB): 0.467\n",
      "            positive  negative  neutral\n",
      "positive    0.742     0.042     0.216     \n",
      "negative    0.100     0.633     0.267     \n",
      "neutral     0.401     0.127     0.472     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test2.txt (MultinomialNB): 0.546\n",
      "            positive  negative  neutral\n",
      "positive    0.745     0.046     0.209     \n",
      "negative    0.161     0.576     0.263     \n",
      "neutral     0.394     0.103     0.503     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test2.txt (GaussianNB): 0.546\n",
      "            positive  negative  neutral\n",
      "positive    0.577     0.065     0.358     \n",
      "negative    0.297     0.407     0.297     \n",
      "neutral     0.462     0.105     0.433     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test2.txt (GaussianNB): 0.476\n",
      "            positive  negative  neutral\n",
      "positive    0.550     0.085     0.365     \n",
      "negative    0.333     0.410     0.257     \n",
      "neutral     0.483     0.133     0.384     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test2.txt (PassiveAggressiveClassifier): 0.513\n",
      "            positive  negative  neutral\n",
      "positive    0.666     0.069     0.265     \n",
      "negative    0.146     0.622     0.232     \n",
      "neutral     0.401     0.105     0.494     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test2.txt (PassiveAggressiveClassifier): 0.522\n",
      "            positive  negative  neutral\n",
      "positive    0.804     0.039     0.157     \n",
      "negative    0.140     0.686     0.174     \n",
      "neutral     0.405     0.105     0.491     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test3.txt (MultinomialNB): 0.436\n",
      "            positive  negative  neutral\n",
      "positive    0.694     0.062     0.245     \n",
      "negative    0.126     0.606     0.268     \n",
      "neutral     0.344     0.156     0.500     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test3.txt (MultinomialNB): 0.493\n",
      "            positive  negative  neutral\n",
      "positive    0.676     0.072     0.252     \n",
      "negative    0.174     0.528     0.298     \n",
      "neutral     0.347     0.133     0.520     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test3.txt (GaussianNB): 0.494\n",
      "            positive  negative  neutral\n",
      "positive    0.498     0.094     0.409     \n",
      "negative    0.253     0.370     0.378     \n",
      "neutral     0.348     0.183     0.469     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test3.txt (GaussianNB): 0.442\n",
      "            positive  negative  neutral\n",
      "positive    0.470     0.118     0.412     \n",
      "negative    0.238     0.446     0.315     \n",
      "neutral     0.346     0.190     0.464     \n",
      "\n",
      "TV\n",
      "semeval-tweets/twitter-test3.txt (PassiveAggressiveClassifier): 0.442\n",
      "            positive  negative  neutral\n",
      "positive    0.633     0.090     0.278     \n",
      "negative    0.164     0.555     0.281     \n",
      "neutral     0.324     0.157     0.519     \n",
      "\n",
      "CV\n",
      "semeval-tweets/twitter-test3.txt (PassiveAggressiveClassifier): 0.419\n",
      "            positive  negative  neutral\n",
      "positive    0.771     0.078     0.151     \n",
      "negative    0.151     0.613     0.235     \n",
      "neutral     0.346     0.144     0.510     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for filename in filenames:\n",
    "    test_tv, test_cv, test_y, test_id = testPpeparation(filename, flag = 3)\n",
    "    for i in range(len(classifiers)):\n",
    "        clf = classifiers[i]\n",
    "        print(\"TV\")\n",
    "        clf.fit(traindata_tv, train_y)\n",
    "        pred_tv = clf.predict(test_tv)\n",
    "        pred_dict = {}\n",
    "        for j in range(len(pred_tv)):\n",
    "            id_ = test_id[j]\n",
    "            pred_dict[ str(id_) ] = pred_tv[j]\n",
    "        #print(pred_dict)   \n",
    "        evaluation.evaluate(pred_dict, filename, classifiers_names[i])\n",
    "        evaluation.confusion(pred_dict, filename, classifiers_names[i])\n",
    "        print(\"CV\")\n",
    "\n",
    "        clf.fit(traindata_cv, train_y)\n",
    "        pred_cv = clf.predict(test_cv)\n",
    "        pred_dict = {}\n",
    "        for j in range(len(pred_cv)):\n",
    "            id_ = test_id[j]\n",
    "            pred_dict[str(id_)] = pred_cv[j]\n",
    "\n",
    "        evaluation.evaluate(pred_dict, filename, classifiers_names[i])\n",
    "        evaluation.confusion(pred_dict, filename, classifiers_names[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 15ms/step\n",
      "semeval-tweets/twitter-test1.txt (model): 0.521\n",
      "            positive  negative  neutral\n",
      "positive    0.722     0.071     0.208     \n",
      "negative    0.142     0.689     0.169     \n",
      "neutral     0.274     0.150     0.576     \n",
      "\n",
      "58/58 [==============================] - 1s 16ms/step\n",
      "semeval-tweets/twitter-test2.txt (model): 0.527\n",
      "            positive  negative  neutral\n",
      "positive    0.755     0.053     0.193     \n",
      "negative    0.133     0.733     0.133     \n",
      "neutral     0.392     0.105     0.503     \n",
      "\n",
      "75/75 [==============================] - 1s 15ms/step\n",
      "semeval-tweets/twitter-test3.txt (model): 0.498\n",
      "            positive  negative  neutral\n",
      "positive    0.710     0.091     0.199     \n",
      "negative    0.165     0.636     0.199     \n",
      "neutral     0.333     0.125     0.542     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in filenames:\n",
    "    X_test, y_test, test_id = testPpeparation(filename, 2)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "    y_test = change(y_test)\n",
    "    y_pred = model.predict(X_test, verbose=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    pred_dict = {}\n",
    "    for j in range(len(y_pred)):\n",
    "        id_ = test_id[j]\n",
    "        if y_pred[j] == 1:\n",
    "            pred_dict[ str(id_) ] = 'negative'\n",
    "        if y_pred[j] == 2:\n",
    "            pred_dict[ str(id_) ] = 'positive'\n",
    "        if y_pred[j] == 0:\n",
    "            pred_dict[ str(id_) ] = 'neutral'\n",
    "\n",
    "    evaluation.evaluate(pred_dict, filename, \"model\")\n",
    "    evaluation.confusion(pred_dict, filename, \"model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
